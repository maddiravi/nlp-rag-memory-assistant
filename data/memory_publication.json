[
    {
        "id": "0CBAR8U8FakE",
        "username": "3rdson",
        "license": "none",
        "title": "How to Add Memory to RAG Applications and AI Agents",
        "publication_description": "![1705674621330.png](1705674621330.png)\n\nSometime in the last 5 months, I built a RAG application, and after building this RAG application, I realised there was a need to add memory to it before moving it to production. I went on YouTube and searched for videos, but I couldn\u2019t find anything meaningful. I saw some videos, but these videos weren\u2019t about adding persistent memory to a production-ready RAG application. They only talked about adding in-memory storage to a RAG application, which is unsuitable for a full-scale application.\n\nIt was then that I realized I needed to figure things out myself and write a good article that would guide readers through the thought processes and steps needed to add memory to a RAG application or AI agent.\n\nQuick Note: If you are building with Streamlit, you can follow this tutorial to find an easy way to add memory to your Streamlit app.\n\n\n---\n\nPre-requisites :\n\n1. Before jumping into the discussion, I want to believe you already know what RAG is and why it is needed. If you\u2019re unfamiliar with this concept, you can read more about it [here](https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag).\n\n2. I also want to believe you already know how to build RAG applications. If you want to learn how to build RAG applications, you can follow my [previous article](https://app.readytensor.ai/publications/how_to_build_rag_apps_with_pinecone_openai_langchain_and_python_sBFzhbX4GpeQ).\n\n3. For this tutorial, I used MongoDB as my traditional database, Langchain as my LLM framework and OpenAI GPT 3.5turbo as my LLM. But you can use any technologies of your choice once you have understood the workflow.\n\n4. To follow along, `pip install` the libraries below.\n\n```\nopenai\npython-dotenv \nlangchain-openai\npymongo\n```\n---\n## Now you are good to go\n![1-5e5944a1.png](1-5e5944a1.png)\n---\n# What Is Memory and Why Do RAG Applications and AI Agents Need Them?\n\nLet\u2019s use ChatGPT as an example. When you ask ChatGPT a question like `\u201cWho is the current president of America?\u201d`, it will tell you \u2018`\u2018Joe Biden\u201c` and then if you go further to ask \u201cHow old is he?\u201c, ChatGPT will tell you `\u201c88\u201d.`\n\nNow, here is the question: \u201cHow was chatGPT able to relate the second question to the first question and give you the answer you needed without you being so specific in your question?\u201d The simple answer to this is the presence of memory.\n\nJust like the same way human beings can easily relate to past experiences or questions, ChatGPT has been built to have memory which can help it know when you are asking a question related to the previous question.\n\nIn my simplest definition, and with regards to RAG and AI agents, memory or adding memory to RAG applications means making the AI agent to be able to make inferences from previous questions and give you new answers based on new questions, previous questions and previous answers.\n\nSo now that you have known what memory is, the question is:\n\nHow Can I Add a Memory to My RAG or AI Agent?\n\nHere is the concept I came up with.\n\nHuman beings have memory because they all have a brain that stores information, and they can answer and make decisions based on the information(data) stored in their brains.\n\nSo to achieve this when building an AI Agent or an RAG application, you need to also give the RAG application a brain by including the following:\n\n1. A database (for storing user\u2019s questions, the AI\u2019s answer, chat IDs, the user\u2019s email etc)\n\n2. A function that retrieves users\u2019 previous questions whenever a new question is asked\n\n3. A function that uses LLM to check if the current question is related to the previous one. If it is related, it will create a new stand-alone question using the present question and previous questions. This question will now be embedded and sent to the vector database or AI agent, depending on what you are building.\nBut if the present question is not related to the past questions, it will send the question as it is.\n\n## Creating a Database for Storing the User\u2019s Questions and AI\u2019s Answers\n\nBelow, I used pymongo to create a Mongo database so you can have an understanding of the kind of fields you will need.\n\n```python\nfrom pymongo import MongoClient\nfrom datetime import datetime\nfrom bson.objectid import ObjectId\n\n# Connect to MongoDB (modify the URI to match your setup)\nclient = MongoClient(\"mongodb://localhost:27017/\")\ndb = client[\"your_database_name\"] # The name of your database\ncollection = db[\"my_ai_application\"] # The name of the collection\n\n# Sample document to be inserted\ndocument = {\n    \"_id\": ObjectId(\"66c990f566416e871fdd0b43\"),  # you can omit this to auto-generate\n    \"question\": \"Who is the President of America?\",\n    \"email\": \"nnajivictorious@gmail.com\",\n    \"response\": \"The current president of the United States is Joe Biden.\",\n    \"chatId\": \"52ded9ebd9ac912c8433b699455eb655\",\n    \"userId\": \"6682632b88c6b314ce887716\",\n    \"isActive\": True,\n    \"isDeleted\": False,\n    \"createdAt\": datetime(2024, 8, 24, 7, 51, 17, 503000),\n    \"updatedAt\": datetime(2024, 8, 24, 7, 51, 17, 503000)\n}\n\n# Insert the document into the collection\nresult = collection.insert_one(document)\nprint(f\"Inserted document with _id: {result.inserted_id}\")\n```\nIn the code above, I created a MongoDB connection using MongoClient and connected to a specified database and collection in MongoDB. I then defined a sample document with fields like `question`, `email`, `response`, `chatId`, and `userId`, along with metadata fields such as` isActive`, `isDeleted`, `createdAt`, and `updatedAt` to track each entry's status and timestamps.\n\nThe _id field is assigned using ObjectId, which you can omit to let MongoDB auto-generate it. When insert_one(document) is called, the document is inserted into the `my_ai_application` collection, and MongoDB returns a unique _id for the document, which is printed to confirm the insertion.\n\nMake sure you change your connection credentials and other specific information.\n\nNow that you have created the database and have understood the kind of fields you need in the database, let\u2019s now see how to use the database to create a memory.\n\n## Creating a Function That Retrieves Users\u2019 Previous Questions Whenever a New Question Is Asked\n\nBelow, we are going to define a function that retrieves the user\u2019s last 3 questions from the database using the user\u2019s email and the chat_id.\n\n```python\nfrom typing import List\n\nclient = MongoClient(\"mongodb://localhost:27017/\")\ndb = client.your_database_name\ncollection = db.my_ai_application\n# no need to initialize this connection if you had already done it\n\ndef get_last_three_questions(email: str, chat_id: str) -> List[str]:\n    \"\"\"\n    Retrieves the last three questions asked by a user in a specific chat session.\n\n    Args:\n        email (str): The user's email address used to filter results.\n        chat_id (str): The unique identifier for the chat session.\n\n    Returns:\n        List[str]: A list containing the last three questions asked by the user,\n                   ordered from most recent to oldest.\n    \"\"\"\n    query = {\"email\": email, \"chatId\": chat_id}\n    results = collection.find(query).sort(\"createdAt\", -1).limit(3)\n    questions = [result[\"question\"] for result in results]\n    return questions\n\n\n# Call the function\npast_questions = get_last_three_questions(\"nnajivictorious@gmail.com\", \"52ded9ebd9ac912c8433b699455eb655\")\n```\n\nYou can change this to retrieve the last five or even ten questions from the user\u2019s database by setting `.limit(5)` or `.limit(10).`\n\nBut note: These questions, together with the new question will still be passed into a system prompt later. So, you need to make sure you aren\u2019t exceeding the input token size of your LLM.\n\nNow that you have defined a function that retrieves the past questions from the database, you need to create a new function that compares the current question with the previous questions and creates a stand-alone question if needed.\n\nBut if the new question has nothing to do with the previous questions, it will push the user\u2019s question just as it is.\n\nCreating a function that creates a standalone question by comparing the new question with the previous questions\n\nBelow we are going to create a system prompt called new_question_modifier and now use this system prompt within the function we will define.\nIt is this system prompt that does the comparing for us.\n\nCheck the code below to understand how it works.\n\n```python\nfrom langchain_openai import OpenAI \nfrom dotenv import load_dotenv\n\n# Load your OpenAI API key from .env file\nload_dotenv()\nCHAT_LLM = OpenAI()\n\nnew_question_modifier = \"\"\"\nYour primary task is to determine if the latest question requires context from the chat history to be understood.\n\nIMPORTANT: If the latest question is standalone and can be fully understood without any context from the chat history or is not related to the chat history, you MUST return it completely unchanged. Do not modify standalone questions in any way.\n\nOnly if the latest question clearly references or depends on the chat history should you reformulate it as a complete, standalone legal question. When reformulating:\n\n\"\"\"\n\ndef modify_question_with_memory(new_question: str, past_questions: List[str]) -> str:\n    \"\"\"\n    Modifies a new question by incorporating past questions as context.\n\n    This function takes a new question and a list of past questions, combining them\n    into a single prompt for the language model (LLM) to generate a standalone question\n    with sufficient context. If there are no past questions, the new question is returned as-is.\n\n    Args:\n        new_question (str): The latest question asked.\n        past_questions (List[str]): A list of past questions for context.\n\n    Returns:\n        str: A standalone question that includes necessary context from past questions.\n    \"\"\"\n    if past_questions:\n        past_questions_text = \" \".join(past_questions)\n        # Combine the system prompt with the past questions and the new question\n        system_prompt = f\"{new_question_modifier}\\nChat history: {past_questions_text}\\nLatest question: {new_question}\"\n        # Get the standalone question using the LLM\n        standalone_question = CHAT_LLM.invoke(system_prompt)\n    else:\n        standalone_question = new_question\n\n    return standalone_question\n\n\nmodified_question = modify_question_with_memory(new_question=\"your new question here\", past_questions=past_questions)\n```\nThe code above creates a stand-alone question using the previous questions, the new question, and the new_question_modifier which is passed into an LLM (OpenAI)\n\n- But what do I really mean by a standalone question?\n\nA stand-alone question is a question that can be understood by the LLM without prior knowledge of the past conversation.\nLet me explain with an example\u2026\u2026\n\nLet\u2019s assume your first question is, `\u201cWho is the president of America?\u201d` and the LLM answers `\u201cJoe Biden\u201d` and then you ask `\u201cHow old is he?\u201d`\n\nThe question, `how old is he?` is not a standalone question because no one can answer the question without knowing whom you are talking about.\n\nSo what the function above does is: It will look at your new question `\u201cHow old is he?\u201c` and compare it with the former question `\u201cWho is the president of America?\u201c`\n\nThen the LLM will ask itself, \u201cIs the recent question related to the past questions?\u201c\n\nIf the answer is yes, it will now modify this new question to something like `\u201cHow old is the current president of America?\u201c` or `\u201cHow old is Joe Biden?\u201c` and then return this new question so that it will now be embedded and sent to the vector database for similarity search.\n\nBut if the answer is no, it will pass your question just as it is.\n\nThis modified question is called a `stand-alone question` because anyone can understand it even without knowing the previous conversation.\n\nI hope this is clear \ud83d\ude01\u270c\ufe0f\n\nFinally, after the function has given you the standalone question, you can now send it to your embedding model and from there to your vector store for similarity search\n\nNote: All these steps must be in a single pipeline so that the output of one becomes the input of the next until the user gets his answers. I believe you understand what I\u2019m saying \ud83e\udd17\n\nAlso, don\u2019t forget to try out different system prompts and know what works best for your use case. The system prompt I used here is just an example for you to build on.\n\n- IN CONCLUSION\n\nI developed this approach after thorough brainstorming, and while it works effectively for the most part, I\u2019d genuinely appreciate any feedback you have. I'd also be grateful if you could share any alternative approaches you've tried that might improve upon it. See you in the comment section and thank you so much for reading\n\nHAPPY RAGING\ud83e\udd17\ud83d\ude80\n\nYou can always reach me on\n\n[X: 3rdSon__](https://x.com/3rdSon__)\n\n[LinkedIn: Victory Nnaji](https://www.linkedin.com/in/3rdson/)\n\n[GitHub: 3rd-Son](https://github.com/3rd-Son)--DIVIDER--"
    }
]